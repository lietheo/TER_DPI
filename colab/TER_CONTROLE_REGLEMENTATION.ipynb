{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Vérification des attributs des couches contenues dans le GPKG POINT = Cotation** (py8) ArcGIS Pro\n",
        "\n",
        "Ce programme vérifie l'ensemble des couches du GPKG pour savoir si les attributs PROF et Zreseau sont présents (donc positifs), puis les extrait dans des shapefiles individuels par couche."
      ],
      "metadata": {
        "id": "c9AhDxxr_cZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image_py8](../images_colab/py8.png)"
      ],
      "metadata": {
        "id": "1KiiUHMMDKEr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfYY_cov3FBM"
      },
      "outputs": [],
      "source": [
        "import arcpy\n",
        "import os\n",
        "\n",
        "gpkg_path = r\"C:\\Users\\liege\\Desktop\\TER_SHAPES\\POINT\\POINT.gpkg\" #gpkg POINT\n",
        "dossier_sortie = r\"C:\\Users\\liege\\Downloads\\POINT_ATTRIBUT\"\n",
        "nom_champ_prof = \"PROF\"\n",
        "nom_champ_zreseau = \"Zreseau\"\n",
        "\n",
        "arcpy.env.workspace = gpkg_path\n",
        "couches = arcpy.ListFeatureClasses()\n",
        "\n",
        "for couche in couches:\n",
        "    champs = [f.name for f in arcpy.ListFields(couche)]\n",
        "    if nom_champ_prof not in champs or nom_champ_zreseau not in champs:\n",
        "        print(f\"La couche {couche} ne contient pas les champs {nom_champ_prof} et {nom_champ_zreseau}.\")\n",
        "        continue\n",
        "\n",
        "    # Expression pour sélectionner les entités avec PROF < 0 ou Zreseau < 0\n",
        "    expression = f'\"{nom_champ_prof}\" < 0 OR \"{nom_champ_zreseau}\" < 0'\n",
        "\n",
        "    # Création d’une couche temporaire avec cette sélection\n",
        "    couche_temp = \"temp_layer\"\n",
        "    arcpy.MakeFeatureLayer_management(couche, couche_temp, expression)\n",
        "\n",
        "    # Vérifier si la couche temporaire contient des entités\n",
        "    count = int(arcpy.GetCount_management(couche_temp).getOutput(0))\n",
        "    if count == 0:\n",
        "        print(f\" Aucune erreur d'attribut dans la couche {couche}\")\n",
        "        arcpy.Delete_management(couche_temp)\n",
        "        continue\n",
        "\n",
        "    # Construire le chemin de sortie avec mention \"erreurs_ATTRIBUTS\"\n",
        "    nom_sortie = f\"erreurs_ATTRIBUTS_{couche}.shp\"\n",
        "    sortie_path = os.path.join(dossier_sortie, nom_sortie)\n",
        "\n",
        "    # Exporter les entités sélectionnées\n",
        "    arcpy.CopyFeatures_management(couche_temp, sortie_path)\n",
        "    arcpy.Delete_management(couche_temp)\n",
        "\n",
        "    print(f\"Exporté les entités avec erreurs d'attributs dans : {sortie_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vérification des attributs IDRegard des couches contenues dans le GPKG SYMBOLE, pour les réseaux EP et EU (les seuls avec des ID)** (py9) ArcGIS Pro\n",
        "\n",
        "Ce programme vérifie les 2 couches du GPKG afin de détecter si un attribut IDRegard a été attribué à deux entités différentes, puis extrait ces entités dans des shapefiles individuels par couche. (Pour certains symboles, c’est normal.)"
      ],
      "metadata": {
        "id": "u9NSVfsiEmgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image_py9](../images_colab/py9.png)"
      ],
      "metadata": {
        "id": "JwWYg5eZEm09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import arcpy\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "gpkg_path = r\"C:\\Users\\liege\\Desktop\\TER_SHAPES\\SYMBOLE\\SYMBOLE.gpkg\" #gpkg SYMBOLE\n",
        "dossier_sortie = r\"C:\\Users\\liege\\Downloads\\SYMBOLE_ATTRIBUT\"\n",
        "champ_id = \"IDRegard\"\n",
        "\n",
        "couches = [\n",
        "    \"Symboles_EP_EtiArcGIS\",\n",
        "    \"Symboles_EU_EtiArcGIS\"\n",
        "]\n",
        "\n",
        "arcpy.env.workspace = gpkg_path\n",
        "os.makedirs(dossier_sortie, exist_ok=True)\n",
        "\n",
        "def clean_export_shapefile(input_layer, output_path):\n",
        "    \"\"\"Exporte en shapefile en supprimant les champs non compatibles\"\"\"\n",
        "    if not arcpy.Exists(input_layer):\n",
        "        raise RuntimeError(f\"La couche {input_layer} n'existe pas.\")\n",
        "\n",
        "    fields = arcpy.ListFields(input_layer)\n",
        "    shp_types = ['Integer', 'SmallInteger', 'Double', 'Single', 'String', 'Date']\n",
        "\n",
        "    # Création d'un FieldMappings pour ne garder que les champs compatibles\n",
        "    field_mappings = arcpy.FieldMappings()\n",
        "    for field in fields:\n",
        "        if field.type in shp_types and len(field.name) <= 10:\n",
        "            fm = arcpy.FieldMap()\n",
        "            fm.addInputField(input_layer, field.name)\n",
        "            field_mappings.addFieldMap(fm)\n",
        "\n",
        "    # Export propre\n",
        "    arcpy.FeatureClassToFeatureClass_conversion(\n",
        "        input_layer,\n",
        "        os.path.dirname(output_path),\n",
        "        os.path.basename(output_path),\n",
        "        field_mapping=field_mappings\n",
        "    )\n",
        "\n",
        "for couche in couches:\n",
        "    print(f\"\\n Traitement de la couche : {couche}\")\n",
        "\n",
        "    if not arcpy.Exists(couche):\n",
        "        print(f\"La couche {couche} n'existe pas dans le gpkg.\")\n",
        "        continue\n",
        "\n",
        "    champs = [f.name for f in arcpy.ListFields(couche)]\n",
        "    if champ_id not in champs:\n",
        "        print(f\"Le champ '{champ_id}' est absent dans {couche}.\")\n",
        "        continue\n",
        "\n",
        "    # Récupérer tous les ID non nuls\n",
        "    ids = []\n",
        "    with arcpy.da.SearchCursor(couche, [champ_id]) as cursor:\n",
        "        for row in cursor:\n",
        "            if row[0] is not None:\n",
        "                ids.append(row[0])\n",
        "\n",
        "    # Chercher les ID en doublon\n",
        "    compteur = Counter(ids)\n",
        "    ids_dupliques = [id_val for id_val, count in compteur.items() if count > 1]\n",
        "\n",
        "    if not ids_dupliques:\n",
        "        print(f\"Pas d'ID doublon dans la couche {couche}.\")\n",
        "        continue\n",
        "\n",
        "    # Construire expression SQL adaptée (en fonction du type champ)\n",
        "    field_obj = arcpy.ListFields(couche, champ_id)[0]\n",
        "    if field_obj.type in ['String', 'Guid']:\n",
        "        expression = \" OR \".join([f\"{champ_id} = '{val}'\" for val in ids_dupliques])\n",
        "    else:\n",
        "        expression = \" OR \".join([f\"{champ_id} = {val}\" for val in ids_dupliques])\n",
        "\n",
        "    temp_layer = \"temp_dups\"\n",
        "    # Supprimer si existe déjà\n",
        "    if arcpy.Exists(temp_layer):\n",
        "        arcpy.Delete_management(temp_layer)\n",
        "\n",
        "    arcpy.MakeFeatureLayer_management(couche, temp_layer, where_clause=expression)\n",
        "\n",
        "    count = int(arcpy.GetCount_management(temp_layer).getOutput(0))\n",
        "    if count == 0:\n",
        "        print(f\"Aucun doublon trouvé après filtrage pour {couche}.\")\n",
        "        arcpy.Delete_management(temp_layer)\n",
        "        continue\n",
        "\n",
        "    sortie_path = os.path.join(dossier_sortie, f\"duplicates_{couche}.shp\")\n",
        "\n",
        "    # Exporter en shapefile nettoyé des champs incompatibles\n",
        "    clean_export_shapefile(temp_layer, sortie_path)\n",
        "\n",
        "    arcpy.Delete_management(temp_layer)\n",
        "    print(f\"Exporté les entités avec ID doublons dans : {sortie_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "NxvqQOviEnJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Création d’un gpkg avec une seule couche contenant les buffers selon les classes de réseaux** (py10) COLAB\n",
        "\n",
        "Ce programme va lire le géopackage des tronçons et va fusionner l’ensemble des entités dans un seul fichier si elles contiennent une classe. Un champ avec le nom des couches sera créé. Trois types de buffer possibles : classe A = 0,5 m, classe B = 1,5 m et classe C = 3 m."
      ],
      "metadata": {
        "id": "uHvkcQeCdXAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image_py10](../images_colab/py10.png)"
      ],
      "metadata": {
        "id": "Fy3qTUhadXXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import fiona\n",
        "from shapely.geometry import Polygon\n",
        "from pyproj import CRS\n",
        "\n",
        "gpkg_path = \"/content/LINEAIRE.gpkg\"  #gpkg LINEAIRE\n",
        "\n",
        "output_folder = \"buffers_output\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "output_gpkg = os.path.join(output_folder, \"buffers_classes.gpkg\")\n",
        "\n",
        "def is_metric_crs(crs):\n",
        "    try:\n",
        "        crs_obj = CRS(crs)\n",
        "        unit = crs_obj.axis_info[0].unit_name.lower()\n",
        "        return \"metre\" in unit or \"meter\" in unit\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "buffer_sizes = {\n",
        "    \"A\": 0.5,\n",
        "    \"B\": 1.5,\n",
        "    \"C\": 3.0\n",
        "}\n",
        "\n",
        "layers = fiona.listlayers(gpkg_path)\n",
        "print(f\"Couches détectées : {layers}\")\n",
        "\n",
        "buffer_list = []\n",
        "\n",
        "for layer_name in layers:\n",
        "    print(f\"Traitement de la couche : {layer_name}\")\n",
        "    gdf = gpd.read_file(gpkg_path, layer=layer_name)\n",
        "\n",
        "    if 'classe' not in gdf.columns and 'Classe' in gdf.columns:\n",
        "        gdf = gdf.rename(columns={'Classe': 'classe'})\n",
        "    if 'classe' not in gdf.columns:\n",
        "        print(f\" - Pas de champ 'classe' dans {layer_name}, couche ignorée.\")\n",
        "        continue\n",
        "\n",
        "    gdf_sel = gdf[gdf['classe'].isin(buffer_sizes.keys())].copy()\n",
        "    if gdf_sel.empty:\n",
        "        print(f\" - Pas d'entités des classes A, B, C dans {layer_name}\")\n",
        "        continue\n",
        "\n",
        "    gdf_sel['layer'] = layer_name\n",
        "\n",
        "    if not is_metric_crs(gdf_sel.crs):\n",
        "        print(\"  CRS non métrique détecté, reprojection en UTM...\")\n",
        "        gdf_sel = gdf_sel.to_crs(gdf_sel.estimate_utm_crs())\n",
        "\n",
        "    def buffer_by_class(row):\n",
        "        dist = buffer_sizes.get(row['classe'], 0)\n",
        "        return row.geometry.buffer(dist)\n",
        "\n",
        "    gdf_sel['geometry'] = gdf_sel.apply(buffer_by_class, axis=1)\n",
        "    gdf_sel['buffer_size'] = gdf_sel['classe'].map(buffer_sizes)\n",
        "\n",
        "    keep_fields = ['classe', 'layer', 'buffer_size', 'geometry']\n",
        "    drop_cols = [col for col in gdf_sel.columns if col not in keep_fields]\n",
        "    gdf_sel = gdf_sel.drop(columns=drop_cols)\n",
        "\n",
        "    buffer_list.append(gdf_sel)\n",
        "\n",
        "if buffer_list:\n",
        "    final_gdf = gpd.GeoDataFrame(pd.concat(buffer_list, ignore_index=True), crs=buffer_list[0].crs)\n",
        "    final_gdf.to_file(output_gpkg, layer='buffers_classes', driver=\"GPKG\")\n",
        "    print(f\"Buffers sauvegardés dans {output_gpkg}\")\n",
        "else:\n",
        "    print(\"Aucune entité à bufferiser.\")\n"
      ],
      "metadata": {
        "id": "4Iszxj3QdXq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Création d’un GPKG avec une seule couche contenant les tronçons trop proches d’autres tronçons de type différent (< 20 cm)** (py11) COLAB\n",
        "\n",
        "Ce programme va lire le géopackage des tronçons et va sélectionner puis fusionner dans une même couche les tronçons de types différents proches de moins de 20 cm."
      ],
      "metadata": {
        "id": "5XzjdP5AzC63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image_py11](../images_colab/py11.png)"
      ],
      "metadata": {
        "id": "cMY7PrdOzDQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import fiona\n",
        "\n",
        "gpkg_path = \"/content/LINEAIRE.gpkg\"  # gpkg LINEAIRE\n",
        "\n",
        "layers = fiona.listlayers(gpkg_path)\n",
        "print(f\"Couches détectées : {layers}\")\n",
        "\n",
        "gdfs = []\n",
        "for layer in layers:\n",
        "    gdf = gpd.read_file(gpkg_path, layer=layer)\n",
        "    # Vérifier que c'est un GeoDataFrame avec géométrie et que ce n'est pas vide\n",
        "    if hasattr(gdf, \"geometry\") and not gdf.empty:\n",
        "        # Vérifier que la première géométrie existe et est un LineString/MultiLineString\n",
        "        if gdf.geometry.iloc[0] is not None and gdf.geometry.iloc[0].geom_type in ['LineString', 'MultiLineString']:\n",
        "            gdf['source_layer'] = layer\n",
        "            gdfs.append(gdf)\n",
        "\n",
        "if not gdfs:\n",
        "    raise RuntimeError(\"Aucune couche linéaire trouvée dans ce GeoPackage.\")\n",
        "\n",
        "merged = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True), crs=gdfs[0].crs)\n",
        "print(f\"Total entités fusionnées : {len(merged)}\")\n",
        "\n",
        "merged['buffer'] = merged.geometry.buffer(0.2)\n",
        "\n",
        "sindex = merged.sindex\n",
        "\n",
        "close_indices = set()\n",
        "for idx, buffered_geom in merged['buffer'].items():\n",
        "    possible_matches = list(sindex.intersection(buffered_geom.bounds))\n",
        "    if idx in possible_matches:\n",
        "        possible_matches.remove(idx)\n",
        "\n",
        "    current_layer = merged.at[idx, 'source_layer']\n",
        "\n",
        "    for other_idx in possible_matches:\n",
        "        if merged.at[other_idx, 'source_layer'] != current_layer:\n",
        "            if buffered_geom.intersects(merged.at[other_idx, 'buffer']):\n",
        "                close_indices.add(idx)\n",
        "                close_indices.add(other_idx)\n",
        "\n",
        "print(f\"Nombre d'entités trop proches (<20cm) entre différentes couches : {len(close_indices)}\")\n",
        "\n",
        "proches = merged.loc[list(close_indices)].copy()\n",
        "proches.drop(columns=['buffer'], inplace=True)\n",
        "\n",
        "output_path = \"entites_trop_proches.gpkg\"\n",
        "proches.to_file(output_path, layer=\"entites_proches\", driver=\"GPKG\")\n",
        "\n",
        "print(f\"Export terminé vers : {output_path}\")\n"
      ],
      "metadata": {
        "id": "JrQPynXFzDh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vérification de la profondeur des réseaux d’un point de vue légal (information de cotation)** (py12) COLAB\n",
        "\n",
        "Ce programme va lire les géopackages qui permettent de coter les tronçons et va comparer le champ PROF ou ZTampon - Zradier aux valeurs légales de profondeur actuelles (en ne gardant que les entités ayant les champs requis complets)."
      ],
      "metadata": {
        "id": "WQJvgVXbqbhO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image_py12](../images_colab/py12.png)"
      ],
      "metadata": {
        "id": "_U17AVrJqb8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "import fiona\n",
        "import os\n",
        "\n",
        "# Dictionnaire des profondeurs légales par type réseau (majuscule)\n",
        "profondeur_legale = {\n",
        "    \"AEP\": 1.0,\n",
        "    \"EP\": 1.0,\n",
        "    \"EU\": 1.0,\n",
        "    \"SLT\": 0.6,\n",
        "    \"HTA\": 0.6,\n",
        "    \"ECL\": 0.6,\n",
        "    \"BT\": 0.6,\n",
        "    \"HYD\": 0.7\n",
        "}\n",
        "\n",
        "point_gpkg = \"/content/POINT.gpkg\" # gpkg POINT\n",
        "symbole_gpkg = \"/content/SYMBOLE.gpkg\" # gpkg SYMBOLE\n",
        "\n",
        "def get_existing_field(fields, candidates):\n",
        "    \"\"\"Retourne le premier champ existant dans la liste candidates, ou None.\"\"\"\n",
        "    for c in candidates:\n",
        "        if c in fields:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def detect_non_conformes(gdf, profondeur_fields, type_field, est_symbole=False):\n",
        "    \"\"\"Filtre les entités avec profondeur non conforme selon la réglementation.\"\"\"\n",
        "    non_conformes = []\n",
        "    for idx, row in gdf.iterrows():\n",
        "        type_reseau = row.get(type_field)\n",
        "        if not isinstance(type_reseau, str):\n",
        "            continue\n",
        "        type_reseau = type_reseau.upper()\n",
        "\n",
        "        if type_reseau not in profondeur_legale:\n",
        "            # Type non reconnu, on ignore l’entité\n",
        "            continue\n",
        "        seuil = profondeur_legale[type_reseau]\n",
        "\n",
        "        if est_symbole:\n",
        "            ztampon = row.get(profondeur_fields[0], None)\n",
        "            zradier = row.get(profondeur_fields[1], None)\n",
        "            if ztampon in [None, 0] or zradier in [None, 0]:\n",
        "                continue\n",
        "            try:\n",
        "                prof = float(ztampon) - float(zradier)\n",
        "            except:\n",
        "                continue\n",
        "            if prof <= 0:\n",
        "                continue\n",
        "        else:\n",
        "            prof = row.get(profondeur_fields[0], None)\n",
        "            if prof is None:\n",
        "                continue\n",
        "            try:\n",
        "                prof = float(prof)\n",
        "            except:\n",
        "                continue\n",
        "            if prof < 0:\n",
        "                continue\n",
        "\n",
        "        if prof < seuil:\n",
        "            non_conformes.append(row)\n",
        "\n",
        "    if non_conformes:\n",
        "        return gpd.GeoDataFrame(non_conformes, columns=gdf.columns)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "rapport = []\n",
        "\n",
        "# --- Analyse POINT.gpkg ---\n",
        "print(\"Analyse du GeoPackage POINT...\")\n",
        "point_layers = fiona.listlayers(point_gpkg)\n",
        "\n",
        "for layer in point_layers:\n",
        "    print(f\"Traitement couche POINT : {layer}\")\n",
        "    gdf = gpd.read_file(point_gpkg, layer=layer)\n",
        "\n",
        "    champ_prof = get_existing_field(gdf.columns, [\"PROF\", \"Prof\", \"prof\"])\n",
        "    champ_type = get_existing_field(gdf.columns, [\"TYPE\", \"Type\", \"type\"])\n",
        "    if champ_prof is None or champ_type is None:\n",
        "        print(f\"Champs PROF ou TYPE absents dans la couche {layer}, passage.\")\n",
        "        continue\n",
        "\n",
        "    result = detect_non_conformes(gdf, [champ_prof], champ_type, est_symbole=False)\n",
        "    if result is not None:\n",
        "        result['source_layer'] = layer\n",
        "        rapport.append(result)\n",
        "\n",
        "# --- Analyse SYMBOLE.gpkg ---\n",
        "print(\"\\nAnalyse du GeoPackage SYMBOLE...\")\n",
        "symbole_layers = [\"Symboles_EU_EtiArcGIS\", \"Symboles_EP_EtiArcGIS\"]\n",
        "\n",
        "for layer in symbole_layers:\n",
        "    print(f\"Traitement couche SYMBOLE : {layer}\")\n",
        "    gdf = gpd.read_file(symbole_gpkg, layer=layer)\n",
        "\n",
        "    champ_type = get_existing_field(gdf.columns, [\"TYPE\", \"Type\", \"type\"])\n",
        "    champ_ztampon = get_existing_field(gdf.columns, [\"ZTampon\", \"Ztampon\", \"ztampon\"])\n",
        "    champ_zradier = get_existing_field(gdf.columns, [\"ZRadier\", \"Zradier\", \"zradier\"])\n",
        "\n",
        "    if champ_type is None or champ_ztampon is None or champ_zradier is None:\n",
        "        print(f\"Champs TYPE, ZTampon ou ZRadier absents dans la couche {layer}, passage.\")\n",
        "        continue\n",
        "\n",
        "    result = detect_non_conformes(gdf, [champ_ztampon, champ_zradier], champ_type, est_symbole=True)\n",
        "    if result is not None:\n",
        "        result['source_layer'] = layer\n",
        "        rapport.append(result)\n",
        "\n",
        "# --- Export shapefile dans Colab (répertoire courant) ---\n",
        "if rapport:\n",
        "    import pandas as pd\n",
        "    rapport_total = pd.concat(rapport, ignore_index=True)\n",
        "\n",
        "    sortie = \"rapport_profondeur_non_conforme.shp\"\n",
        "    rapport_total.to_file(sortie, driver=\"ESRI Shapefile\")\n",
        "    print(f\"\\nRapport généré : {sortie}\")\n",
        "else:\n",
        "    print(\"\\nAucune entité non conforme détectée.\")\n"
      ],
      "metadata": {
        "id": "boTN_H5MqcPV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}